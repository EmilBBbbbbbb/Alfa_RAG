# search_rag.py
"""
–ó–∞–≥—Ä—É–∂–∞–µ—Ç Chroma DB –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤.
–í–æ–ø—Ä–æ—Å—ã –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è —á–µ—Ä–µ–∑ Ollama (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω).
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è, –∏ web_list –≤—Å–µ–≥–¥–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç 5 —Å–∞–π—Ç–æ–≤.
"""

import os
import pandas as pd
from tqdm import tqdm
import re
from random import sample

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama

from langchain_core.prompts import ChatPromptTemplate

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
MODEL_NAME = "ai-forever/ru-en-RoSBERTa"
DB_DIR = "chroma_db"
INPUT_Q = "data/questions_clean.csv"
OUTPUT = "RAG_results_llm.csv"
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "mistral")
OLLAMA_BASE_URL = os.environ.get("OLLAMA_HOST", "http://localhost:11434")

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
QUERY_PROMPT = (
    "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –æ—á–∏—Å—Ç–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n"
    "–ó–∞–¥–∞—á–∞: –≤–µ—Ä–Ω—É—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–π, –Ω–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–π –∏ —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∑–∞–ø—Ä–æ—Å–∞.\n"
    "–ü—Ä–∞–≤–∏–ª–∞:\n"
    "1) –£–¥–∞–ª–∏ —ç–º–æ–¥–∑–∏, HTML, URL, email, —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã.\n"
    "2) –†–∞—Å–∫—Ä–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–†–§ -> –†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è –∏ —Ç.–ø.).\n"
    "3) –°–æ—Ö—Ä–∞–Ω–∏ —Å–º—ã—Å–ª –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.\n"
    "4) –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –æ—á–∏—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.\n\n"
    "–ó–∞–ø—Ä–æ—Å:\n{text}\n\n"
    "–û—Ç–≤–µ—Ç:"
)


def make_llm():
    """–°–æ–∑–¥–∞—ë—Ç LLM Ollama, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞"""
    try:
        return ChatOllama(model=OLLAMA_MODEL, base_url=OLLAMA_BASE_URL, temperature=0)
    except Exception as e:
        print("[WARN] Ollama init failed:", e)
        return None


def regex_clean_query(q: str) -> str:
    """–ë—ã—Å—Ç—Ä–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ä–µ–≥—É–ª—è—Ä–∫–∞–º–∏"""
    if not isinstance(q, str):
        return ""
    q = q.replace("\\n", " ").replace("\\t", " ")
    q = re.sub(r'https?://\S+|www\.\S+', ' ', q)
    q = re.sub(r'\S*@\S*\s?', ' ', q)
    q = re.sub(r'<.*?>', ' ', q)
    q = re.sub(r'[\u2022\u25CF\uf0a7‚Ä¢]+', ' ', q)
    q = re.sub(r'[^–∞-—èa-z0-9\s\-,]', ' ', q, flags=re.IGNORECASE)
    q = re.sub(r'\s+', ' ', q).strip()
    return q


def llm_clean_query(text: str, llm) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Ollama (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
    if not isinstance(text, str) or not text.strip():
        return ""
    prompt = QUERY_PROMPT.format(text=text)
    try:
        resp = llm.invoke([{"role": "user", "content": prompt}])
        cleaned = resp.content.strip()
        return cleaned if cleaned else regex_clean_query(text)
    except Exception:
        return regex_clean_query(text)


def main():
    print("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ Chroma DB...")
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)
    db = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)

    print("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤...")
    qdf = pd.read_csv(INPUT_Q)
    print(f"–í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(qdf)}")

    llm = make_llm()

    # –û—á–∏—Å—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    cleaned_queries = []
    for q in tqdm(qdf["query"].fillna(""), desc="–û—á–∏—Å—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"):
        if llm:
            cleaned_queries.append(llm_clean_query(q, llm))
        else:
            cleaned_queries.append(regex_clean_query(q))

    qdf["query_cleaned"] = cleaned_queries

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ web_id –≤ –±–∞–∑–µ ‚Äî –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è –¥–ª—è fallback
    all_docs = db.get()
    all_web_ids = [
        meta.get("web_id")
        for meta in all_docs["metadatas"]
        if meta and "web_id" in meta
    ]
    unique_web_ids = list(set(all_web_ids))

    print("üîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫...")
    results = []
    for _, row in tqdm(qdf.iterrows(), total=len(qdf), desc="–ü–æ–∏—Å–∫"):
        q_id = row["q_id"]
        query = (row.get("query_cleaned") or "").strip()

        # fallback ‚Äî –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—É—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º regex –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
        if not query:
            query = regex_clean_query(row.get("query", ""))

        try:
            docs = db.similarity_search(query, k=5)
            top_web_list = [d.metadata.get("web_id") for d in docs if d.metadata.get("web_id")]
        except Exception:
            top_web_list = []

        # fallback –µ—Å–ª–∏ Chroma –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª
        if not top_web_list:
            if unique_web_ids:
                top_web_list = sample(unique_web_ids, min(5, len(unique_web_ids)))
            else:
                top_web_list = []

        results.append({"q_id": q_id, "web_list": top_web_list})

    out = pd.DataFrame(results)
    out.to_csv(OUTPUT, index=False)
    print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤", OUTPUT)


if __name__ == "__main__":
    main()
